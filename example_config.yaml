# basic settings
task_name: med_mcqa # bigbench | ncbi | ... | or your own task
search_algo: mcts # mcts | beam_search
print_log: true
log_dir: ./logs/

# your initial prompt
init_prompt: |
  Answer questions from real world medical exams.

task_setting:
  train_size: 2000 
  eval_size: 5 # data split for reward calculation
  test_size: 5 # if test_size is not 0, the optimized nodes will be tested at last.
  seed: 42 # if need to fixed shuffled dataset
  data_dir: MedMCQA # if data is downloaded
  # Note: the current supported bigbench tasks are specified by 
  # data_dir using the same task_name (bigbench), if there is not
  # specific .py class inplemented in the tasks folder.
  post_instruction: false # false: prompt + task question | true: task question + prompt

base_model_setting:
  model_type: openbiollm_8b # openai | palm | hf_text2text | hf_textgeneration | ct_model
  model_name: Bio-Medical-Llama-3-2-1B-CoT-012025 # api-based model'name or huggingface model name
  temperature: 0.6
  api_key: null # if need api key
  device: cuda:0 # cuda | cpu | cuda:x, e.g. 0,1,2...
  gpu_ids: null # [0,1,...] for vllm model
  model_path: null # ct model requires the downloaded model's path

optim_model_setting:
  model_type: openbiollm_8b # openai | palm | hf_text2text | hf_textgeneration | ct_model
  model_name: Bio-Medical-Llama-3-2-1B-CoT-012025 # api-based model'name or huggingface model name
  temperature: 0.6
  api_key: null  # if need api key
  device: cuda:1 # cuda | cpu | cuda:x, e.g. 0,1,2...
  gpu_ids: null # [0,1,...] for vllm model
  model_path: null # ct model requires the downloaded model's path

search_setting:
  iteration_num: 1
  expand_width: 1 # num of branches of each node
  depth_limit: 3 # the max depth of mcts
  # mcts setting
  min_depth: 2 # min depth of mcts for early stop
  w_exp: 2.5 # balance exploration and exploitation
  # beam search setting
  beam_width: 3

world_model_setting:
  # mcts world model setting
  train_shuffle: true
  num_new_prompts: 1 # 3 if beam search
  train_batch_size: 1
  
